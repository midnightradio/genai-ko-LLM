{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2662eb8f-ba8b-4c34-8ab9-b312ecfcff91",
   "metadata": {},
   "source": [
    "# Korean LLM (Large Language Model) Serving on SageMaker with AWS Large Model Container DLC\n",
    "---\n",
    "\n",
    "한국어 LLM 모델 SageMaker 서빙 핸즈온 (No inference code)\n",
    "\n",
    "- LLM GitHub: https://github.com/nlpai-lab/KULLM\n",
    "- Hugging Face model hub: https://huggingface.co/nlpai-lab/kullm-polyglot-5.8b-v2\n",
    "- [AWS Blog: Deploy large models on Amazon SageMaker using DJLServing and DeepSpeed model parallel inference](https://aws.amazon.com/ko/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a056d19-3339-4778-b73d-f5fe14d50ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('../templates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b73dc3e-82d3-4e08-983f-75ec6c83c6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU boto3 huggingface_hub sagemaker langchain deepspeed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ca4169-bc8f-4eeb-b897-b48b15f1c6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker, boto3, jinja2\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_client = boto3.client(\"s3\")  # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "jinja_env = jinja2.Environment()  # jinja environment to generate model configuration templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85236aea-d0e6-4b5f-89bf-2d185881740c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1. Download LLM model and upload it to S3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c958f9dc-9917-4f62-8036-b7cde5cda405",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 code prefix \n",
      " ko-llms/serving/kullm-polyglot-12-8b-v2/code\n",
      "S3 model prefix: \n",
      " ko-llms/serving/kullm-polyglot-12-8b-v2/model\n",
      "S3 model artifact path: \n",
      " s3://sagemaker-us-west-2-339967300945/ko-llms/serving/kullm-polyglot-12-8b-v2/model\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "model_id = \"nlpai-lab/kullm-polyglot-12.8b-v2\"\n",
    "model_prefix = model_id.split('/')[-1].replace('.', '-')\n",
    "model_tar_dir = f\"/home/ec2-user/SageMaker/models/{model_prefix}\"\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\"]\n",
    "\n",
    "if not os.path.isdir(model_tar_dir):\n",
    "    os.makedirs(model_tar_dir, exist_ok=True)\n",
    "    # - Leverage the snapshot library to donload the model since the model is stored in repository using LFS    \n",
    "    snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        local_dir=str(model_tar_dir), \n",
    "        local_dir_use_symlinks=False,        \n",
    "        allow_patterns=allow_patterns,\n",
    "        cache_dir=\"/home/ec2-user/SageMaker/\"        \n",
    "    )\n",
    "\n",
    "bucket_prefix = 'ko-llms/serving'    \n",
    "s3_code_prefix = f\"{bucket_prefix}/{model_prefix}/code\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = f\"{bucket_prefix}/{model_prefix}/model\"  # folder where model checkpoint will go\n",
    "s3_model_artifact = f\"s3://{bucket}/{s3_model_prefix}\"\n",
    "\n",
    "print(f\"S3 code prefix \\n {s3_code_prefix}\")\n",
    "print(f\"S3 model prefix: \\n {s3_model_prefix}\")\n",
    "print(f\"S3 model artifact path: \\n {s3_model_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c326a83c-348d-4b99-8258-b87f24316011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws configure set default.s3.max_concurrent_requests 100\n",
    "aws configure set default.s3.max_queue_size 10000\n",
    "aws configure set default.s3.multipart_threshold 1GB\n",
    "aws configure set default.s3.multipart_chunksize 64MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b84012c-0777-47ee-a88c-1d98b6fbd938",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to --- > s3://sagemaker-us-west-2-339967300945/ko-llms/serving/kullm-polyglot-12-8b-v2/model\n",
      "We will set option.s3url=s3://sagemaker-us-west-2-339967300945/ko-llms/serving/kullm-polyglot-12-8b-v2/model\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync {model_tar_dir} {s3_model_artifact}\n",
    "print(f\"Model uploaded to --- > {s3_model_artifact}\")\n",
    "print(f\"We will set option.s3url={s3_model_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fca204-ff77-49c5-bc76-b5383c8ded4e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Model Serving Scripts\n",
    "---\n",
    "### Create `serving.properties`\n",
    "\n",
    "이 설정 파일은 어떤 추론 최적화 라이브러리를 사용할지, 어떤 설정을 사용할지 DJL Serving에 알려주는 설정 파일입니다. 필요에 따라 적절한 구성을 설정할 수 있습니다.\n",
    "\n",
    "There are a few options specified here. Lets go through them in turn<br>\n",
    "1. `engine` - specifies the engine that will be used for this workload. In this case we'll be hosting a model using the **FasterTransformer**\n",
    "2. `option.entryPoint` - specifies the entrypoint code that will be used to host the model. `djl_python.fastertransformer` refers to the `fastertransformer.py` module from [djl_python repo](https://github.com/deepjavalibrary/djl-serving/tree/master/engines/python/setup/djl_python).  \n",
    "3. `option.s3url`: Set this to the URI of the Amazon S3 bucket that contains the model. When this is set, the container leverages [s5cmd](https://github.com/peak/s5cmd) to download the model from s3. This enables faster deployments by utilizing optimized approach within the DJL inference container to transfer the model from S3 into the hosting instance \n",
    "4. `option.tensor_parallel_degree`: Set to the number of GPU devices over which DeepSpeed needs to partition the model. This parameter also controls the number of workers per model which will be started up when DJL serving runs. As an example if we have a 8 GPU machine and we are creating 8 partitions then we will have 1 worker per model to serve the requests.\n",
    "\n",
    "`serving.properties`의 일반적인 설정법과 자세한 내용은 https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html 를 참조하세요.\n",
    "\n",
    "<img src=\"./images/Slide21.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e960108-f6bf-4b8d-9ea6-78b6ca3c6915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_path = f\"src/{model_prefix}\"\n",
    "!rm -rf {src_path}\n",
    "os.makedirs(src_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5beab7e1-205c-47b1-b27a-0120614caec2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/kullm-polyglot-12-8b-v2/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile {src_path}/serving.properties\n",
    "engine = FasterTransformer\n",
    "option.entryPoint = djl_python.fastertransformer\n",
    "option.s3url = {{s3url}}\n",
    "option.tensor_parallel_degree = 4\n",
    "option.dtype = fp16\n",
    "option.task = text-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8cacee-600c-460b-a0b1-7c263a26b5c7",
   "metadata": {},
   "source": [
    "### serving.properties의 S3 경로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34f03823-aa2a-48e8-b862-116917c1cc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t\u001b[36mengine\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mFasterTransformer\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     2\t\u001b[36moption.entryPoint\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mdjl_python.fastertransformer\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     3\t\u001b[36moption.s3url\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33ms3://sagemaker-us-west-2-339967300945/ko-llms/serving/kullm-polyglot-12-8b-v2/model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     4\t\u001b[36moption.tensor_parallel_degree\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33m4\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     5\t\u001b[36moption.dtype\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mfp16\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     6\t\u001b[36moption.task\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mtext-generation\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(f\"{src_path}/serving.properties\").open().read())\n",
    "Path(f\"{src_path}/serving.properties\").open(\"w\").write(template.render(s3url=s3_model_artifact))\n",
    "!pygmentize {src_path}/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a063b634-3a4f-49ec-8779-fdda2818326e",
   "metadata": {},
   "source": [
    "### Create the Tarball and then upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90fa1063-839a-46be-9ca4-f6b9c14efbc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./serving.properties\n"
     ]
    }
   ],
   "source": [
    "!rm -rf model.tar.gz\n",
    "!tar czvf model.tar.gz -C {src_path} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d60fd8a-8a0e-497c-8628-98eb26772cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-339967300945/ko-llms/serving/kullm-polyglot-12-8b-v2/code/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")\n",
    "#!rm -rf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea2358-3ba1-4ff3-9ca4-df772b59770d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Serve LLM Model on SageMaker\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820b1c3-7854-433d-bbb6-03193abefa22",
   "metadata": {},
   "source": [
    "### Create SageMaker Model\n",
    "\n",
    "SageMaker 엔드포인트 생성 매개변수 VolumeSizeInGB를 지정할 때 마운트되는 Amazon EBS(Amazon Elastic Block Store) 볼륨에 /tmp를 매핑하기 때문에 컨테이너는 인스턴스의 `/tmp` 공간에 모델을 다운로드합니다. 이때 s5cmd (https://github.com/peak/s5cmd) 를 활용하므로 대용량 모델을 빠르게 다운로드할 수 있습니다.\n",
    "볼륨 인스턴스와 함께 미리 빌드되어 제공되는 p4dn과 같은 인스턴스의 경우 컨테이너의 `/tmp`를 계속 활용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "598b6ded-ba9c-4f25-b862-090546607b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.23.0-fastertransformer5.3.0-cu118\n",
      "kullm-polyglot-12-8b-v2-2023-09-06-04-40-28-722\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker import image_uris\n",
    "\n",
    "img_uri = image_uris.retrieve(framework=\"djl-fastertransformer\", region=region, version=\"0.23.0\")\n",
    "model_name = name_from_base(f\"{model_prefix}\")\n",
    "print(f\"Image going to be used is ---- > {img_uri}\")\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b94cef46-9058-4c8e-8006-17cc8a5cb553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-339967300945/ko-llms/serving/kullm-polyglot-12-8b-v2/code/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")\n",
    "env = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\", \"TRANSFORMERS_CACHE\": \"/tmp\"}\n",
    "\n",
    "model = Model(image_uri=img_uri, model_data=s3_code_artifact, env=env, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96783b7-9e6a-4bed-8ff9-c779d9e628e4",
   "metadata": {},
   "source": [
    "### Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed3941cd-4ade-4e8e-8d33-6becb07fa82d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kullm-polyglot-12-8b-v2-djl-ft-2023-09-06-04-40-45-340\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "endpoint_name = name_from_base(f\"{model_prefix}-djl-ft\")\n",
    "print(endpoint_name)\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=3600,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf78f45-b06e-431c-9048-3ade776cac07",
   "metadata": {},
   "source": [
    "엔드포인트가 생성되는 동안 아래의 문서를 같이 확인해 보세요.\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6122a3f8-78b6-42b9-b390-af8942d8e30c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b> [SageMaker LLM Serving] <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints/kullm-polyglot-12-8b-v2-djl-ft-2023-09-06-04-40-45-340\">Check Endpoint Status</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "def make_console_link(region, endpoint_name, task='[SageMaker LLM Serving]'):\n",
    "    endpoint_link = f'<b> {task} <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\">Check Endpoint Status</a></b>'   \n",
    "    return endpoint_link\n",
    "\n",
    "endpoint_link = make_console_link(region, endpoint_name)\n",
    "display(HTML(endpoint_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b80c4b8b-21f4-4954-9b6b-68f8385ec202",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kullm-polyglot-12-8b-v2-djl-ft-2023-09-06-04-40-45-340'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccfc30ad-d2a0-47ea-92cb-3d99d784fe53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint is  InService\n",
      "CPU times: user 20.6 ms, sys: 4.41 ms, total: 25 ms\n",
      "Wall time: 195 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from inference_lib import describe_endpoint, Prompter\n",
    "describe_endpoint(endpoint_name)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b26c2e-b620-4df9-b712-c5aeb8e9e32a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Inference\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d857c2d0-bb29-4214-bb12-9e75342d3f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from inference_lib import Prompter\n",
    "prompter = Prompter(\"kullm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97a9a12f-bdcc-498a-a132-9b99441c3662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from inference_lib import KoLLMSageMakerEndpoint\n",
    "\n",
    "ep = KoLLMSageMakerEndpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7e616af-f748-4582-b8f6-8fd34de042a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"return_full_text\": True,\n",
    "    \"temperature\": 0.01,\n",
    "    \"top_p\": 0.9,\n",
    "    \"return_full_text\": False,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"presence_penalty\": None,\n",
    "    \"eos_token_id\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "instruction = \"아래 질문에 100글자 이상으로 자세하게 대답해줘.\"\n",
    "#instruction = \"\"\n",
    "input_text = \"고려대학교에 대해서 알려줘\"\n",
    "prompt = prompter.generate_prompt(instruction, input_text)\n",
    "payload = {\n",
    "    \"inputs\": [prompt,],\n",
    "    \"parameters\": params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18dee518-297a-41ac-aab4-c05498b36802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Response: AWS는 Amazon Web Services의 약자입니다. 클라우드 컴퓨팅 서비스를 제공하는 선도 기술 회사입니다. '\n",
      " 'AWS는 전 세계 고객에게 인프라, 애플리케이션 및 서비스를 제공하는 데 사용되는 클라우드 컴퓨팅 플랫폼입니다. AWS는 고객이 클라우드 '\n",
      " '컴퓨팅을 통해 애플리케이션와 애플리케이션을 관리하고 확장할 수 있도록 지원하. AWS는 고객mazon Web Services의 클라우드 '\n",
      " '컴퓨팅 플랫폼을 통해 고객이 컴퓨팅, 스토리지, 네트워킹, 데이터베이스, 보안 등 다양한 서비스를 사용할 수 있는 Amazon Web '\n",
      " 'Services의 클라우드 컴퓨팅 플랫폼을 통해 고객이 컴퓨팅, 스토리지, 네트워킹, 데이터베이스, 보안 등 다양한 서비스를 사용할 수 '\n",
      " '있습니다. AWS는 고객이 클라우드 컴퓨팅을 통해 데이터를 저장하고 관리할 수 있도록 지원합니다. AWS는 고객이 클라우드 컴퓨팅을 통해 '\n",
      " '데이터를 저장하고 관리할 수 있도록 지원합니다. AWS는 고객이 클라우드 컴퓨팅을 통해 데이터를 저장하고 관리할 수 있도록 지원합니다. '\n",
      " 'AWS는 고객이 클라우드 컴퓨팅을 통해 데이터를 저장하고 관리할 수 있도록 지원합니다. AWS는 고객이 클라우드 컴퓨팅을 통해 데이터를 '\n",
      " '저장하고 관리할 수 있도록 지원합니다. AWS는 고객이 클라우드 컴퓨팅을 통해 데이터를 저장하고 관리할 수 있도록 '\n",
      " '지원합니다.<|endoftext|>[이투데이 최환 기자][\"경제 경제 회복에 대한 신뢰감 회복\"] 미국의 주간 신규 실업수당 청구건수가 '\n",
      " '예상보다 큰 폭으로 감소한 것으로 나타났다. 미 노동부는 지난주 신규 실업수당 청구건수가 전주대비 7000만건 감소한 47만8000건을 '\n",
      " '기록했다고 밝혔다. 이는 전문가 예상치인 47만건건을 상회하는 수치다. 그러나 이를 나타내는 4주 평균 청구건수는 47만6750건으로 '\n",
      " '전주대비 2000건 증가했다. 이는 또 연속수당 청구건수가 전주대비 3000건 감소한 47만5000건을 기록했다고 밝혔다. 이는 전문가 '\n",
      " '예상치인 47만5000건을 상회하는 수치다. 노동부는 또 연속 실업수당 청구건수가 전주대비 1000건 감소한 447만건을 기록했다고 '\n",
      " '밝혔다. 이는 전문가 예상치인 447만건을 상회하는 수치다. 노동부는 또 연속 실업수당 청구건수가 전주대비 1000건 감소한 447만건을 '\n",
      " '기록했다고 밝혔다')\n",
      "('Response: AWS는 Amazon Web Services의 약자입니다. 클라우드 컴퓨팅 서비스를 제공하는 글로벌 기술 회사입니다. '\n",
      " 'AWS는 전 세계 고객에게 인프라, 애플리케이션, 서비스를 제공하는 데 사용되는 Amazon Web Services의AWS)의 '\n",
      " '일부입니다. AWS는 컴퓨팅, 스토리지, 네트워킹, 데이터 분석, 애플리케이션 관리, 애플리케이션 호스팅, 서비스 관리, 보안 등 다양한 '\n",
      " '서비스를 제공합니다. AWS는 전 세계 고객에게 서비스를 제공하기 위해 전 세계 여러 지역에 위치한 여러 데이터 센터를 보유하고 '\n",
      " '있습니다. AWS는 고객이 클라우드 컴퓨팅의 이점을 활용하여 비용 효율적이고 확장 가능한 방식으로 컴퓨팅, 스토리지, 네트워킹, '\n",
      " '애플리케이션을 사용할 수 있도록 지원합니다.<|endoftext|>[머니투데이 황보람 기자] [[the300]\"국민의당, '\n",
      " '\\'국민의당\\'으로 당명 변경…안철수 대표가 대표직 맡아\"]본문 이미지 영역안철수 새정치민주연합 의원/사진=뉴스1안철수 새정치민주연합 '\n",
      " '의원이 국민의당 창당을 앞두고 탈당을 선언했다. 안 의원은 \"국민의당 창당을 통해 정치정치민주연합을 떠나기로 했다\"고 밝혔다.안 의원은 '\n",
      " '15일 오전 국회 정론관에서 기자회견을 열고 \"더 이상 합리적 개혁을이 분열하는 일은 없어야 한다\"며 \"국민의당 창당을 통해 낡은 정치를 '\n",
      " '바꾸고, 국민의 삶을 돌보는 정치를 바꾸고, 대한민국의 미래를 준비는 정치를 바꾸겠다\"고 말했다.안 의원은 \"국민의당이은 정치 변화를 '\n",
      " '바라는 국민의 열망에 대한 응답\"이라며 \"국민의당이 변화의 삶에 희망을 드릴 수 있는 정당이 될 수 있도록 최선을 다하겠다\"고 '\n",
      " '덧붙였했다.국민의당은 이날 오전 창당준비위원회를 발족하고 오 작업에 본격 돌입했다. 창당준비위원장은 안철수 의원이 맡았다. 안 의원은 '\n",
      " '\"국민의당 창당은 낡은 정치와의 결별이며 국민의 삶을 바꾸는 정치의 시작\"이라고 말했다.국민의당은 이날는 17일 중앙당 창당대회를 열고 '\n",
      " \"창당을 완료화할 예정이다.[내 삶을 바꾸는 정치뉴스 'the 300' 바로가기]['스페셜 걸' 포토][손안의 경제\")\n",
      "('Response: AWS는 Amazon Web Services의 약자입니다. 클라우드 컴퓨팅 서비스를 제공하는 글로벌 기술 회사입니다. '\n",
      " 'AWS는 전 세계 고객에게 인프라, 애플리케이션, 서비스를 제공하는 데 사용되는 클라우드 컴퓨팅 플랫폼입니다. AWS는 고객이 '\n",
      " '클라우드에서 서비스를 쉽게 관리하고, 확장하고, 비용을 절감할 수 있도록 지원합니다. AWS는 Amazon Web Services의 '\n",
      " '약자입니다.<|endoftext|>[머니투데이 김경환 기자][[2008위기 10년]②금융위기 이후 금융산업의 미래는]본문 이미지 '\n",
      " '영역\"금융위기 이후 금융산업의 미래는 어떻게 될 것인가?\" 금융위기 이후 10년이 지난 지금,산업의 미래에 대한 질문은 다시 제기되고 '\n",
      " '있다. 금융위기 이후 금융산업은 규제 강화, 저금리, 기술 침체 등의 영향으로 성장세가 둔화되고 수익성이도 악화됐다. 그러나 금융산업은 '\n",
      " '위기 이후에도 여전히 중요한 경제 부문으로 남아있으며, 금융산업의 미래는 금융산업의 경쟁력 가능성과 금융 시스템의 안정성에 영향을 미칠 '\n",
      " '수 있는 중요한 질문이다. 금융위기 이후 금융산업의 미래에 대한 주요 질문은 다음과 같다.1. 금융산업의 미래는 어떻게 될 것인가?금융. '\n",
      " '금융산업의 미래는 어떻게산업의 지속 가능성에 어떤 영향을 미칠 것인가?3. 금융산업의 미래는 금융 시스템의 안정성에 어떤 영향을 미칠 '\n",
      " '것인가?4. 금융산업의 미래는 금융산업의 경쟁력에 어떤 영향을 미칠 것인가?5. 금융산업의 미래는 금융산업의 혁신에 어떤 영향을 미칠 '\n",
      " '것인가?6. 금융산업의 미래는 금융산업의 일자리 창출에 어떤 영향을 미칠 것인가?7. 금융산업의 미래는 금융산업의 소비자 신뢰에 어떤 '\n",
      " '영향을 미칠 것인가?8. 금융산업의 미래는 금융산업의 사회적 책임에 어떤 영향을 미칠 것인가?9. 금융산업의 미래는 금융산업의 환경 '\n",
      " '가능성에 어떤 영향을 미칠 것인가?10. 금융산업의 미래는 금융산업의 규제에 어떤 영향을 미칠 것인가?11. 금융산업의 미래는 금융산업의 '\n",
      " '혁신에 어떤 영향을 미칠 것인가?12. 금융산업의 미래는 금융산업의 일자리 창출에 어떤 영향을 미칠 것인가?13. 금융산업의 미래는 '\n",
      " '금융산업의 소비자 보호에 어떤 영향을 미칠 것인가?')\n",
      "8.37 s ± 0 ns per loop (mean ± std. dev. of 1 run, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n3 -r1\n",
    "\n",
    "payload = ep.get_payload(instruction, input_text, params)\n",
    "generated_text = ep.infer(payload, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b393ca-7fcc-4f33-973b-1da22ed9c20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ea9f0-1107-4bb1-9d64-5c3aa33562f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3ba52-6abd-4db5-bc77-1a36e4bd204e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d82bc-ad3a-4486-b4d5-3a9b6b7bd52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d7b34-3235-4308-b34a-329db2cb62fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659f78e-4e93-4399-b784-b50131e0ab3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44db971e-ecc1-402c-b813-edbfab82724a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c0593cc-181a-471e-8945-dc627ece22d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.5 ms, sys: 0 ns, total: 17.5 ms\n",
      "Wall time: 3.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '!A: 아마존 웹 서비스(AWS)는 클라우드 컴퓨팅 서비스를 제공하는 회사입니다. 클라우드 컴퓨팅은 인터넷을 통해 컴퓨팅 리소스를 제공하는 것을 말합니다. AWS는 컴퓨팅 리소스를 제공하고, 이를 통해 기업이 애플리케이션과 서비스를 구축하고 실행할 수 있도록 지원합니다.AWS는 전 세계에서 가장 큰 클라우드 컴퓨팅 서비스 제공업체 중 하나이며, 전 세계에서 가장 큰 클라우드 컴퓨팅 회사입니다.AWS는 고객이 AWS를 사용하여 애플리케이션과 서비스를 구축하고 실행할 수 있도록 지원하는 서비스를 제공합니다. AWS는 고객이 AWS를 사용하여 애플리케이션과 서비스를 구축하고 실행할 수 있도록 지원하는 서비스를 제공합니다.AWS는 고객이 AWS를 사용하여 애플리케이션과 서비스를 구축하고 실행할 수 있도록 지원하는 서비스를 제공합니다.AWS는 고객이 AWS를 사용하여 애플리케이션과 서비스를 구축하고 실행할'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predictor.predict(\n",
    "    {\n",
    "        \"inputs\": [\"아마존 웹 서비스에 대해서 자세히 알려줘\"],\n",
    "        \"parameters\": {\"max_seq_len\": 200, \"temperature\": 0.1},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd52a8fe-42f8-46f4-af34-4999af60c983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_seq_len\": 512,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "instruction = \"\"\n",
    "input_text = \"아마존 웹 서비스에 대해서 자세히 알려줘\"\n",
    "payload = ep.get_payload(instruction, input_text, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cce18e21-c4c0-412c-a08d-8bc31c1baeb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type bytes is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/base_predictor.py:177\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    131\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     custom_attributes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the inference from the specified endpoint.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m            as is.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     request_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_request_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_variant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39msagemaker_runtime_client\u001b[38;5;241m.\u001b[39minvoke_endpoint(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_args)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/base_predictor.py:231\u001b[0m, in \u001b[0;36mPredictor._create_request_args\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m custom_attributes:\n\u001b[1;32m    229\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomAttributes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m custom_attributes\n\u001b[0;32m--> 231\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/base_serializers.py:232\u001b[0m, in \u001b[0;36mJSONSerializer.serialize\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mdumps(data\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type bytes is not JSON serializable"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d6bcdb9-b73d-468f-8606-6bf4b2f90a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625410bd-e2e9-4d57-bb35-bddf5cf20301",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5. Clean Up\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f69e5266-ac6d-4ae1-afab-815a8620c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cf1594c-7e7a-4ba5-9ba9-1cb8e1646573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feeb821-db0a-4a48-8550-b0146705b8d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "# References\n",
    "---\n",
    "\n",
    "- Model 정보\n",
    "    - kullm-polyglot-5.8b-v2\n",
    "        - This model is a parameter-efficient fine-tuned version of EleutherAI/polyglot-ko-5.8b on a KULLM v2\n",
    "        - https://huggingface.co/nlpai-lab/kullm-polyglot-5.8b-v2        \n",
    "    - kullm-polyglot-12.8b-v2\n",
    "        - This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KULLM v2\n",
    "        - https://huggingface.co/nlpai-lab/kullm-polyglot-12.8b-v2\n",
    "    - beomi/KoAlpaca-Polyglot-12.8B\n",
    "        - This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KoAlpaca Dataset v1.1b\n",
    "        - https://huggingface.co/beomi/KoAlpaca-Polyglot-12.8B\n",
    "    - EleutherAI/polyglot-ko-12.8b\n",
    "        - Polyglot-Ko-12.8B was trained for 167 billion tokens over 301,000 steps on 256 A100 GPUs with the GPT-NeoX framework. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token.\n",
    "        - License: Apache 2.0\n",
    "        - https://huggingface.co/EleutherAI/polyglot-ko-12.8b      \n",
    "- 코드\n",
    "    - [Boto3](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/pytorch_deploy_large_GPT_model/GPT-J-6B-model-parallel-inference-DJL.ipynb)\n",
    "    - [Python SDK](https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/deepspeed/GPT-J-6B_DJLServing_with_PySDK.ipynb)\n",
    "    - [Kor LLM on SageMaker](https://github.com/gonsoomoon-ml/Kor-LLM-On-SageMaker)\n",
    "    - [AWS Generative AI Workshop for Korean language](https://github.com/aws-samples/aws-ai-ml-workshop-kr/tree/master/genai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
